{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "JPTbLi3wY6Xm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg16_bn\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "from utils.early_stopping import EarlyStopping\n",
        "\n",
        "# Set random seed\n",
        "SEED = 42\n",
        "torch.backends.cudnn.deterministic = True\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\" # Cublas ting\n",
        "\n",
        "torch.hub.set_dir('/cs/student/projects1/2021/izaffar/.cache/torch/hub')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = \"/cs/student/projects1/2021/izaffar/FYP/FYP-MUL/data\"\n",
        "# for filename in os.listdir(DATA_DIR):\n",
        "#     print(filename)\n",
        "\n",
        "# Use GPU\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    model_arch=\"brain_mri\",\n",
        "    mask_type=\"right\",\n",
        "    learning_rate=1e-5,\n",
        "    epochs=50,\n",
        "    val_size=20,\n",
        "    test_size=20,\n",
        "    batch_size=4,\n",
        "    k_folds=5,\n",
        "    criterion=\"MSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_pipeline(run_config):\n",
        "    # make the model, data, and optimization problem\n",
        "    dataset = CustomDataset(data_dir=DATA_DIR, model_arch=run_config[\"model_arch\"], mask_type=run_config[\"mask_type\"], transform=True)\n",
        "\n",
        "    # split off test set\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        list(range(len(dataset))), test_size=(run_config[\"test_size\"]/100), random_state=SEED\n",
        "    )\n",
        "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "    test_loader = make_loader(test_dataset, batch_size=run_config[\"batch_size\"])\n",
        "\n",
        "    # TODO: save best model at checkpoint\n",
        "\n",
        "    all_train_losses, all_val_losses, all_val_mean_diffs, all_val_std_diffs = [], [], [], []\n",
        "\n",
        "    # KFold Cross-validation\n",
        "    kfold = KFold(n_splits=run_config[\"k_folds\"], shuffle=True, random_state=SEED)\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(range(len(train_val_indices)))):\n",
        "        print(f'Fold: {fold+1}')\n",
        "\n",
        "        # tell wandb to get started\n",
        "        with wandb.init(project=\"fyp-mul\", entity=\"imaad-zaffar-ucl\", group=\"experiment_1\", name=f\"fold_{fold+1}\", config=run_config):\n",
        "            # access all HPs through wandb.config, so logging matches execution!\n",
        "            config = wandb.config\n",
        "\n",
        "            # model, train_loader, val_loader, test_loader, criterion, optimizer = make(config)\n",
        "            # print(model)\n",
        "\n",
        "            # and use them to train the model\n",
        "            # train(model, train_loader, val_loader, criterion, optimizer, config)\n",
        "\n",
        "            train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "            val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "\n",
        "            train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
        "            val_loader = make_loader(val_dataset, batch_size=config.batch_size)\n",
        "\n",
        "            model, criterion, optimizer = make(config)\n",
        "            # print(model)\n",
        "\n",
        "            # Train the model\n",
        "            train_losses, val_losses, val_mean_diffs, val_std_diffs = train(model, train_loader, val_loader, criterion, optimizer, fold, config)\n",
        "\n",
        "            all_train_losses.append(train_losses)\n",
        "            all_val_losses.append(val_losses)\n",
        "            all_val_mean_diffs.append(val_mean_diffs)\n",
        "            all_val_std_diffs.append(val_std_diffs)\n",
        "\n",
        "            # Log average performance across folds\n",
        "            # log_average_performance(all_train_losses, all_val_losses, all_val_mean_diffs, all_val_std_diffs)\n",
        "\n",
        "            # and test its final performance\n",
        "            test(model, test_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "def log_average_performance(all_train_losses, all_val_losses, all_val_mean_diffs, all_val_std_diffs):\n",
        "    avg_train_loss = np.mean(all_train_losses, axis=0)\n",
        "    avg_val_loss = np.mean(all_val_losses, axis=0)\n",
        "    avg_val_mean_diff = np.mean(all_val_mean_diffs)\n",
        "    avg_val_std_diff = np.mean(all_val_std_diffs)\n",
        "\n",
        "    wandb.log({\"train_loss_avg\": avg_train_loss,\n",
        "                \"val_loss_avg\": avg_val_loss,\n",
        "                \"val_mean_diff_avg\": avg_val_mean_diff,\n",
        "                \"val_std_diff_avg\": avg_val_std_diff})\n",
        "\n",
        "    print(\"Average Performance Across Folds:\")\n",
        "    print(f\"Avg Train Loss: {avg_train_loss[-1]:.3f}, Avg Val Loss: {avg_val_loss[-1]:.3f}\")\n",
        "    print(f\"Avg Val Mean Diff: {avg_val_mean_diff:.3f}, Avg Val Std Diff: {avg_val_std_diff:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    # train, val, test = get_data(val_size=config.val_size, test_size=config.test_size, model_arch=config.model_arch, mask_type=config.mask_type)\n",
        "    # train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    # val_loader = make_loader(val, batch_size=config.batch_size)\n",
        "    # test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = get_model(config.model_arch, DEVICE)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    if config.criterion == \"MSE\":\n",
        "        criterion = nn.MSELoss()\n",
        "    \n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate)\n",
        "    \n",
        "    return model, criterion, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "x-nT8AqJODX3"
      },
      "outputs": [],
      "source": [
        "def convert_pixels_to_mm(length_pixels, images, required_image_width_pixels=320, scale_factor=0.56525):\n",
        "    image_width_pixels = images.size(2)\n",
        "    length_mm = length_pixels * (required_image_width_pixels / image_width_pixels) * scale_factor\n",
        "    return length_mm\n",
        "\n",
        "def get_length_line_simple(masks):\n",
        "    batch_size, _, image_width, image_height = masks.size()\n",
        "    masks = masks.squeeze(1)\n",
        "    lengths = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        mask = masks[i]  # Select the mask for the current sample\n",
        "        \n",
        "        # Find indices of non-zero elements (where the mask is 1)\n",
        "        nonzero_indices = torch.nonzero(mask, as_tuple=False)\n",
        "        \n",
        "        if len(nonzero_indices) > 0:\n",
        "            # Compute the bounding box from the non-zero indices\n",
        "            min_x = nonzero_indices[:, 1].min().item()\n",
        "            min_y = nonzero_indices[:, 0].min().item()\n",
        "            max_x = nonzero_indices[:, 1].max().item()\n",
        "            max_y = nonzero_indices[:, 0].max().item()\n",
        "\n",
        "            # Calculate the diagonal length of the bounding box\n",
        "            width = max_x - min_x\n",
        "            height = max_y - min_y\n",
        "            diagonal_length = torch.sqrt(torch.tensor(width**2 + height**2, dtype=torch.float32))\n",
        "            lengths.append(diagonal_length)\n",
        "        else:\n",
        "            lengths.append(torch.tensor(0.0))  # If no non-zero elements found, return length 0\n",
        "\n",
        "    return torch.stack(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "GAMwvNh0Y6Xn"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, model_arch, mask_type=\"right\", transform=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.model_arch = model_arch\n",
        "        self.transform = transform\n",
        "\n",
        "        # image dir\n",
        "        self.image_dir = os.path.join(data_dir, \"images_best_slice\")\n",
        "        self.image_files = sorted(os.listdir(self.image_dir))\n",
        "\n",
        "        # mask dir\n",
        "        if mask_type == \"both\":\n",
        "            self.mask_dir = os.path.join(data_dir, \"masks_both\")\n",
        "        elif mask_type == \"right\":\n",
        "            self.mask_dir = os.path.join(data_dir, \"masks_right\")\n",
        "        self.mask_files = sorted(os.listdir(self.mask_dir))\n",
        "\n",
        "        assert len(self.image_files) == len(self.mask_files)\n",
        "\n",
        "    def slice_transform(self, slice_image):\n",
        "        if self.model_arch == \"brain_mri\":\n",
        "            m, s = np.mean(slice_image, axis=(0, 1)), np.std(slice_image, axis=(0, 1))\n",
        "            preprocess = transforms.Compose(\n",
        "                [\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.CenterCrop((256, 256)),\n",
        "                    transforms.Normalize(mean=m, std=s),\n",
        "                    # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                ]\n",
        "            )\n",
        "        input_tensor = preprocess(slice_image)\n",
        "        return input_tensor\n",
        "\n",
        "    def mask_transform(self, mask_image):\n",
        "        if self.model_arch == \"brain_mri\":\n",
        "            preprocess = transforms.Compose(\n",
        "                [\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.CenterCrop((256, 256)),\n",
        "                ]\n",
        "            )\n",
        "        input_tensor = preprocess(mask_image)\n",
        "        return input_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
        "\n",
        "        image = np.load(image_path).astype(np.float32)\n",
        "        mask = np.load(mask_path).astype(np.float32)\n",
        "\n",
        "        # transform\n",
        "        if self.transform:\n",
        "            image = self.slice_transform(image)\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_data(val_size, test_size, model_arch, mask_type):\n",
        "#     dataset = CustomDataset(data_dir=DATA_DIR, model_arch=model_arch, mask_type=mask_type, transform=True)\n",
        "#     train_val_indices, test_indices = train_test_split(\n",
        "#         list(range(len(dataset))), test_size=(test_size/100), random_state=SEED\n",
        "#     )\n",
        "#     train_indices, val_indices = train_test_split(\n",
        "#         list(range(len(train_val_indices))), test_size=(val_size/(100-test_size)), random_state=SEED\n",
        "#     )\n",
        "\n",
        "#     train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "#     val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "#     test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "#     return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=0)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "-hkoLX8-Y6Xo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /cs/student/projects1/2021/izaffar/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        }
      ],
      "source": [
        "brain_unet_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "        in_channels=3, out_channels=1, init_features=32, pretrained=True).to(DEVICE)\n",
        "new_in_channels = 1\n",
        "modified_encoder1_weight = brain_unet_model.encoder1.enc1conv1.weight.data[:, :new_in_channels, :, :]\n",
        "brain_unet_model.encoder1.enc1conv1 = nn.Conv2d(new_in_channels, 32, kernel_size=3, padding=1)\n",
        "brain_unet_model.encoder1.enc1conv1.weight.data = modified_encoder1_weight\n",
        "\n",
        "# Define your new model by extending the loaded model\n",
        "class BrainMRIUNetFC(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(BrainMRIUNetFC, self).__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "        output_features = 256 * 256\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(output_features, 1)  # Fully connected layer with a single node\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the base model\n",
        "        x = self.base_model(x)\n",
        "\n",
        "        # Apply your new layer\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Load the pre-trained model\n",
        "def get_model(model_arch, device=\"cpu\"):\n",
        "    if model_arch == \"brain_mri\":\n",
        "        return BrainMRIUNetFC(base_model=brain_unet_model).to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Model type not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer, fold, config):\n",
        "    train_losses, val_losses = [], []\n",
        "    val_mean_diffs, val_std_diffs = [], []\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for _, (images, masks) in enumerate(train_loader):\n",
        "            loss = train_batch(images, masks, model, optimizer, criterion)\n",
        "            train_loss += loss.item() * len(images)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_diffs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _, (images, masks) in enumerate(val_loader):\n",
        "                loss, diff = val_batch(images, masks, model, criterion)\n",
        "                val_loss += loss.item() * len(images)\n",
        "                val_diffs.append(diff.item())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        val_mean_diffs.append(np.mean(val_diffs))\n",
        "        val_std_diffs.append(np.std(val_diffs))\n",
        "\n",
        "        # Log metrics\n",
        "        wandb.log({\"epoch\": epoch,\n",
        "                    f\"train_loss\": train_loss,\n",
        "                    f\"val_loss\": val_loss,\n",
        "                    f\"val_mean_diff\": val_mean_diffs[-1],\n",
        "                    f\"val_std_diff\": val_std_diffs[-1]},\n",
        "                    step=epoch)\n",
        "        print(f\"Epoch {str(epoch).zfill(3)} - Train: {train_loss:.3f}, Val: {val_loss:.3f}\")\n",
        "\n",
        "    return train_losses, val_losses, val_mean_diffs, val_std_diffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_batch(images, masks, model, optimizer, criterion):\n",
        "    images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "    target_lengths = get_length_line_simple(masks)\n",
        "    target_lengths_mm = convert_pixels_to_mm(target_lengths, masks).to(DEVICE)\n",
        "    \n",
        "    # Forward pass ➡\n",
        "    outputs = model(images).squeeze(1)\n",
        "    loss = criterion(outputs, target_lengths_mm)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def val_batch(images, masks, model, criterion):\n",
        "    images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "    target_lengths = get_length_line_simple(masks)\n",
        "    target_lengths_mm = convert_pixels_to_mm(target_lengths, masks).to(DEVICE)\n",
        "    \n",
        "    # Forward pass ➡\n",
        "    outputs = model(images).squeeze(1)\n",
        "    val_loss = criterion(outputs, target_lengths_mm)\n",
        "    val_diff = torch.mean(torch.abs(outputs - target_lengths_mm))\n",
        "\n",
        "    return val_loss, val_diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_log(train_loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": train_loss}, step=example_ct)\n",
        "    print(f\"Train Loss after {str(example_ct).zfill(5)} examples: {train_loss:.3f}\")\n",
        "\n",
        "def val_log(val_loss, val_diff, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"val_loss\": val_loss, \"val_diff\": val_diff}, step=example_ct)\n",
        "    print(f\"Val Loss after {str(example_ct).zfill(5)} examples: {val_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        diffs = []\n",
        "        total = 0\n",
        "        for images, masks in test_loader:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            targets_length = get_length_line_simple(masks)\n",
        "            targets_length_mm = convert_pixels_to_mm(targets_length, masks).to(DEVICE)\n",
        "\n",
        "            outputs = model(images).squeeze(1)\n",
        "            total += masks.size(0)\n",
        "            diffs.extend(torch.abs(outputs - targets_length_mm).detach().cpu().numpy())\n",
        "        \n",
        "        mean_diff = np.mean(diffs)\n",
        "        std_diff = np.std(diffs)\n",
        "        print(f\"Metrics for {total} test images - Mean Diff: {mean_diff}mm, Std Diff: {std_diff}mm\")\n",
        "\n",
        "        wandb.log({\"mean_diff\": np.mean(diffs), \"std_diff\": np.std(diffs)})\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    # torch.onnx.export(model, images, \"model.onnx\")\n",
        "    # wandb.save(\"model.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/cs/student/projects1/2021/izaffar/FYP/FYP-MUL/model/wandb/run-20240303_004804-zsov5h2y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/zsov5h2y' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/zsov5h2y' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/zsov5h2y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000 - Train: 503.346, Val: 527.411\n",
            "Epoch 001 - Train: 464.153, Val: 436.831\n",
            "Epoch 002 - Train: 379.911, Val: 353.310\n",
            "Epoch 003 - Train: 273.977, Val: 254.643\n",
            "Epoch 004 - Train: 172.369, Val: 165.034\n",
            "Epoch 005 - Train: 97.077, Val: 93.515\n",
            "Epoch 006 - Train: 59.035, Val: 54.722\n",
            "Epoch 007 - Train: 38.132, Val: 38.884\n",
            "Epoch 008 - Train: 32.716, Val: 31.877\n",
            "Epoch 009 - Train: 31.092, Val: 27.369\n",
            "Epoch 010 - Train: 26.370, Val: 31.343\n",
            "Epoch 011 - Train: 19.337, Val: 32.541\n",
            "Epoch 012 - Train: 16.364, Val: 31.388\n",
            "Epoch 013 - Train: 17.573, Val: 34.055\n",
            "Epoch 014 - Train: 12.213, Val: 31.112\n",
            "Epoch 015 - Train: 11.096, Val: 33.656\n",
            "Epoch 016 - Train: 7.912, Val: 34.704\n",
            "Epoch 017 - Train: 7.127, Val: 35.621\n",
            "Epoch 018 - Train: 6.407, Val: 36.699\n",
            "Epoch 019 - Train: 7.086, Val: 36.340\n",
            "Epoch 020 - Train: 7.244, Val: 37.127\n",
            "Epoch 021 - Train: 4.394, Val: 35.656\n",
            "Epoch 022 - Train: 3.566, Val: 35.251\n",
            "Epoch 023 - Train: 5.707, Val: 35.807\n",
            "Epoch 024 - Train: 6.369, Val: 33.010\n",
            "Epoch 025 - Train: 4.624, Val: 33.595\n",
            "Epoch 026 - Train: 4.109, Val: 32.161\n",
            "Epoch 027 - Train: 5.808, Val: 35.525\n",
            "Epoch 028 - Train: 4.180, Val: 38.638\n",
            "Epoch 029 - Train: 3.993, Val: 36.468\n",
            "Epoch 030 - Train: 4.646, Val: 36.759\n",
            "Epoch 031 - Train: 3.378, Val: 35.712\n",
            "Epoch 032 - Train: 2.136, Val: 35.925\n",
            "Epoch 033 - Train: 3.405, Val: 36.950\n",
            "Epoch 034 - Train: 2.357, Val: 38.778\n",
            "Epoch 035 - Train: 3.973, Val: 37.741\n",
            "Epoch 036 - Train: 2.382, Val: 35.050\n",
            "Epoch 037 - Train: 2.457, Val: 35.729\n",
            "Epoch 038 - Train: 2.561, Val: 33.886\n",
            "Epoch 039 - Train: 2.659, Val: 36.365\n",
            "Epoch 040 - Train: 2.746, Val: 38.415\n",
            "Epoch 041 - Train: 2.110, Val: 36.223\n",
            "Epoch 042 - Train: 1.988, Val: 35.486\n",
            "Epoch 043 - Train: 2.307, Val: 35.686\n",
            "Epoch 044 - Train: 2.636, Val: 36.732\n",
            "Epoch 045 - Train: 1.381, Val: 35.158\n",
            "Epoch 046 - Train: 2.345, Val: 37.179\n",
            "Epoch 047 - Train: 2.239, Val: 36.732\n",
            "Epoch 048 - Train: 1.343, Val: 38.738\n",
            "Epoch 049 - Train: 3.507, Val: 35.740\n",
            "Metrics for 21 test images - Mean Diff: 3.7737176418304443mm, Std Diff: 3.302788496017456mm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mean_diff</td><td>▁</td></tr><tr><td>std_diff</td><td>▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_diff</td><td>██▆▅▃▁▁▂▁▁▁▁▁▂▁▁▂▁▂▂▂▁▁▁▂▁▁▂▂▁▂▂▁▁▁▁▂▂▁▂</td></tr><tr><td>val_std_diff</td><td>▅▇▆▄▄▅▅█▅▃▆▃▅▂▂▃▄▄▅▄▄▄▂▅▅▃▃▄▄▄▃▂▅▄▁▄▆▆▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>mean_diff</td><td>3.77372</td></tr><tr><td>std_diff</td><td>3.30279</td></tr><tr><td>train_loss</td><td>3.50688</td></tr><tr><td>val_loss</td><td>35.74012</td></tr><tr><td>val_mean_diff</td><td>5.41038</td></tr><tr><td>val_std_diff</td><td>0.85539</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/zsov5h2y' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/zsov5h2y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240303_004804-zsov5h2y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/cs/student/projects1/2021/izaffar/FYP/FYP-MUL/model/wandb/run-20240303_004920-9fjgs14a</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/9fjgs14a' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/9fjgs14a' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/9fjgs14a</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000 - Train: 450.277, Val: 363.465\n",
            "Epoch 001 - Train: 316.689, Val: 246.258\n",
            "Epoch 002 - Train: 196.458, Val: 147.906\n",
            "Epoch 003 - Train: 107.256, Val: 81.448\n",
            "Epoch 004 - Train: 54.631, Val: 44.180\n",
            "Epoch 005 - Train: 31.428, Val: 32.181\n",
            "Epoch 006 - Train: 21.295, Val: 24.062\n",
            "Epoch 007 - Train: 15.868, Val: 17.318\n",
            "Epoch 008 - Train: 12.019, Val: 14.572\n",
            "Epoch 009 - Train: 13.193, Val: 12.197\n",
            "Epoch 010 - Train: 8.651, Val: 12.072\n",
            "Epoch 011 - Train: 5.842, Val: 10.197\n",
            "Epoch 012 - Train: 8.142, Val: 11.978\n",
            "Epoch 013 - Train: 6.612, Val: 9.508\n",
            "Epoch 014 - Train: 7.209, Val: 8.749\n",
            "Epoch 015 - Train: 5.897, Val: 9.114\n",
            "Epoch 016 - Train: 3.166, Val: 8.874\n",
            "Epoch 017 - Train: 5.873, Val: 16.120\n",
            "Epoch 018 - Train: 3.759, Val: 13.941\n",
            "Epoch 019 - Train: 3.321, Val: 7.439\n",
            "Epoch 020 - Train: 2.905, Val: 11.508\n",
            "Epoch 021 - Train: 3.247, Val: 12.768\n",
            "Epoch 022 - Train: 4.160, Val: 8.338\n",
            "Epoch 023 - Train: 3.805, Val: 6.708\n",
            "Epoch 024 - Train: 2.866, Val: 11.769\n",
            "Epoch 025 - Train: 3.626, Val: 6.727\n",
            "Epoch 026 - Train: 3.464, Val: 10.007\n",
            "Epoch 027 - Train: 3.235, Val: 9.678\n",
            "Epoch 028 - Train: 3.717, Val: 6.119\n",
            "Epoch 029 - Train: 2.720, Val: 9.401\n",
            "Epoch 030 - Train: 1.962, Val: 9.633\n",
            "Epoch 031 - Train: 1.562, Val: 7.093\n",
            "Epoch 032 - Train: 2.033, Val: 7.084\n",
            "Epoch 033 - Train: 1.681, Val: 8.472\n",
            "Epoch 034 - Train: 1.449, Val: 8.887\n",
            "Epoch 035 - Train: 2.380, Val: 11.930\n",
            "Epoch 036 - Train: 1.498, Val: 10.303\n",
            "Epoch 037 - Train: 1.096, Val: 8.410\n",
            "Epoch 038 - Train: 1.665, Val: 7.647\n",
            "Epoch 039 - Train: 2.348, Val: 6.993\n",
            "Epoch 040 - Train: 2.172, Val: 13.249\n",
            "Epoch 041 - Train: 2.120, Val: 6.643\n",
            "Epoch 042 - Train: 1.266, Val: 8.788\n",
            "Epoch 043 - Train: 1.184, Val: 6.517\n",
            "Epoch 044 - Train: 2.025, Val: 8.464\n",
            "Epoch 045 - Train: 0.969, Val: 11.735\n",
            "Epoch 046 - Train: 1.515, Val: 10.948\n",
            "Epoch 047 - Train: 0.862, Val: 9.761\n",
            "Epoch 048 - Train: 1.393, Val: 10.650\n",
            "Epoch 049 - Train: 0.869, Val: 7.270\n",
            "Metrics for 21 test images - Mean Diff: 1.17642343044281mm, Std Diff: 1.6125586032867432mm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mean_diff</td><td>▁</td></tr><tr><td>std_diff</td><td>▁</td></tr><tr><td>train_loss</td><td>█▆▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_diff</td><td>█▇▅▃▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_std_diff</td><td>▆█▅█▅▂▅▄▅▃▄▄▂▄▃▃▄▂▃▂▃▁▄▄▂▃▂▁▂▃▂▃▂▂▃▄▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>mean_diff</td><td>1.17642</td></tr><tr><td>std_diff</td><td>1.61256</td></tr><tr><td>train_loss</td><td>0.86942</td></tr><tr><td>val_loss</td><td>7.27004</td></tr><tr><td>val_mean_diff</td><td>1.98959</td></tr><tr><td>val_std_diff</td><td>1.00654</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_2</strong> at: <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/9fjgs14a' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/9fjgs14a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240303_004920-9fjgs14a/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/cs/student/projects1/2021/izaffar/FYP/FYP-MUL/model/wandb/run-20240303_005032-hswya9rw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/hswya9rw' target=\"_blank\">fold_3</a></strong> to <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/hswya9rw' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/hswya9rw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000 - Train: 407.644, Val: 302.350\n",
            "Epoch 001 - Train: 235.751, Val: 162.594\n",
            "Epoch 002 - Train: 120.137, Val: 69.473\n",
            "Epoch 003 - Train: 57.304, Val: 25.658\n",
            "Epoch 004 - Train: 25.947, Val: 13.077\n",
            "Epoch 005 - Train: 16.815, Val: 7.314\n",
            "Epoch 006 - Train: 11.730, Val: 7.857\n",
            "Epoch 007 - Train: 8.219, Val: 6.076\n",
            "Epoch 008 - Train: 10.080, Val: 3.947\n",
            "Epoch 009 - Train: 4.770, Val: 2.956\n",
            "Epoch 010 - Train: 5.821, Val: 3.811\n",
            "Epoch 011 - Train: 3.731, Val: 2.485\n",
            "Epoch 012 - Train: 4.604, Val: 2.344\n",
            "Epoch 013 - Train: 4.145, Val: 3.113\n",
            "Epoch 014 - Train: 4.033, Val: 2.434\n",
            "Epoch 015 - Train: 3.531, Val: 2.602\n",
            "Epoch 016 - Train: 3.374, Val: 2.412\n",
            "Epoch 017 - Train: 5.138, Val: 1.967\n",
            "Epoch 018 - Train: 5.014, Val: 1.359\n",
            "Epoch 019 - Train: 1.552, Val: 1.739\n",
            "Epoch 020 - Train: 3.740, Val: 2.152\n",
            "Epoch 021 - Train: 2.946, Val: 1.822\n",
            "Epoch 022 - Train: 3.782, Val: 1.703\n",
            "Epoch 023 - Train: 2.777, Val: 2.065\n",
            "Epoch 024 - Train: 2.298, Val: 1.977\n",
            "Epoch 025 - Train: 4.085, Val: 2.856\n",
            "Epoch 026 - Train: 2.795, Val: 1.592\n",
            "Epoch 027 - Train: 2.761, Val: 2.796\n",
            "Epoch 028 - Train: 2.480, Val: 2.013\n",
            "Epoch 029 - Train: 2.025, Val: 1.588\n",
            "Epoch 030 - Train: 2.035, Val: 1.772\n",
            "Epoch 031 - Train: 1.646, Val: 1.884\n",
            "Epoch 032 - Train: 1.438, Val: 1.410\n",
            "Epoch 033 - Train: 1.454, Val: 1.456\n",
            "Epoch 034 - Train: 2.636, Val: 1.334\n",
            "Epoch 035 - Train: 1.460, Val: 2.860\n",
            "Epoch 036 - Train: 1.484, Val: 1.928\n",
            "Epoch 037 - Train: 1.145, Val: 1.802\n",
            "Epoch 038 - Train: 1.267, Val: 1.725\n",
            "Epoch 039 - Train: 1.628, Val: 3.238\n",
            "Epoch 040 - Train: 2.132, Val: 3.014\n",
            "Epoch 041 - Train: 2.279, Val: 3.409\n",
            "Epoch 042 - Train: 1.625, Val: 1.572\n",
            "Epoch 043 - Train: 2.022, Val: 3.860\n",
            "Epoch 044 - Train: 2.528, Val: 1.791\n",
            "Epoch 045 - Train: 1.476, Val: 2.486\n",
            "Epoch 046 - Train: 0.957, Val: 1.686\n",
            "Epoch 047 - Train: 1.198, Val: 1.766\n",
            "Epoch 048 - Train: 0.732, Val: 1.803\n",
            "Epoch 049 - Train: 1.494, Val: 1.480\n",
            "Metrics for 21 test images - Mean Diff: 0.8278893828392029mm, Std Diff: 1.3687812089920044mm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mean_diff</td><td>▁</td></tr><tr><td>std_diff</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_diff</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_std_diff</td><td>█▃▆▃▁▃▂▂▃▁▁▁▁▂▂▂▂▁▁▂▂▂▂▁▂▁▂▂▂▂▂▁▂▁▂▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>mean_diff</td><td>0.82789</td></tr><tr><td>std_diff</td><td>1.36878</td></tr><tr><td>train_loss</td><td>1.49381</td></tr><tr><td>val_loss</td><td>1.47967</td></tr><tr><td>val_mean_diff</td><td>0.91305</td></tr><tr><td>val_std_diff</td><td>0.41215</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_3</strong> at: <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/hswya9rw' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/hswya9rw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240303_005032-hswya9rw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/cs/student/projects1/2021/izaffar/FYP/FYP-MUL/model/wandb/run-20240303_005148-akae24r0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/akae24r0' target=\"_blank\">fold_4</a></strong> to <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/akae24r0' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/akae24r0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000 - Train: 416.407, Val: 308.817\n",
            "Epoch 001 - Train: 233.950, Val: 174.027\n",
            "Epoch 002 - Train: 114.409, Val: 78.518\n",
            "Epoch 003 - Train: 48.383, Val: 30.817\n",
            "Epoch 004 - Train: 21.982, Val: 12.464\n",
            "Epoch 005 - Train: 12.396, Val: 7.742\n",
            "Epoch 006 - Train: 7.878, Val: 5.073\n",
            "Epoch 007 - Train: 6.836, Val: 3.424\n",
            "Epoch 008 - Train: 3.969, Val: 2.879\n",
            "Epoch 009 - Train: 4.570, Val: 2.029\n",
            "Epoch 010 - Train: 3.290, Val: 1.577\n",
            "Epoch 011 - Train: 3.654, Val: 1.328\n",
            "Epoch 012 - Train: 2.848, Val: 1.246\n",
            "Epoch 013 - Train: 3.081, Val: 1.004\n",
            "Epoch 014 - Train: 2.101, Val: 1.103\n",
            "Epoch 015 - Train: 1.782, Val: 1.064\n",
            "Epoch 016 - Train: 2.125, Val: 0.909\n",
            "Epoch 017 - Train: 2.383, Val: 1.764\n",
            "Epoch 018 - Train: 1.845, Val: 1.077\n",
            "Epoch 019 - Train: 1.171, Val: 1.239\n",
            "Epoch 020 - Train: 1.757, Val: 0.997\n",
            "Epoch 021 - Train: 3.640, Val: 0.948\n",
            "Epoch 022 - Train: 1.485, Val: 1.104\n",
            "Epoch 023 - Train: 3.341, Val: 0.724\n",
            "Epoch 024 - Train: 1.392, Val: 1.136\n",
            "Epoch 025 - Train: 1.828, Val: 0.626\n",
            "Epoch 026 - Train: 1.625, Val: 0.747\n",
            "Epoch 027 - Train: 1.496, Val: 0.854\n",
            "Epoch 028 - Train: 1.456, Val: 1.111\n",
            "Epoch 029 - Train: 0.925, Val: 1.063\n",
            "Epoch 030 - Train: 1.146, Val: 1.235\n",
            "Epoch 031 - Train: 0.992, Val: 0.892\n",
            "Epoch 032 - Train: 1.253, Val: 0.755\n",
            "Epoch 033 - Train: 0.749, Val: 0.930\n",
            "Epoch 034 - Train: 0.644, Val: 1.354\n",
            "Epoch 035 - Train: 0.923, Val: 0.844\n",
            "Epoch 036 - Train: 1.059, Val: 2.198\n",
            "Epoch 037 - Train: 1.576, Val: 2.021\n",
            "Epoch 038 - Train: 0.907, Val: 1.225\n",
            "Epoch 039 - Train: 0.613, Val: 1.438\n",
            "Epoch 040 - Train: 0.757, Val: 1.180\n",
            "Epoch 041 - Train: 0.767, Val: 3.130\n",
            "Epoch 042 - Train: 0.729, Val: 1.036\n",
            "Epoch 043 - Train: 0.921, Val: 0.983\n",
            "Epoch 044 - Train: 0.693, Val: 0.805\n",
            "Epoch 045 - Train: 0.622, Val: 0.748\n",
            "Epoch 046 - Train: 0.651, Val: 1.079\n",
            "Epoch 047 - Train: 0.661, Val: 1.212\n",
            "Epoch 048 - Train: 0.658, Val: 1.185\n",
            "Epoch 049 - Train: 0.600, Val: 1.617\n",
            "Metrics for 21 test images - Mean Diff: 1.2690231800079346mm, Std Diff: 1.3526077270507812mm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mean_diff</td><td>▁</td></tr><tr><td>std_diff</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_diff</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_std_diff</td><td>▆█▄▅▃▂▁▂▂▁▂▁▁▁▂▂▁▁▂▁▁▁▂▁▂▂▁▁▁▁▁▁▂▃▂▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>mean_diff</td><td>1.26902</td></tr><tr><td>std_diff</td><td>1.35261</td></tr><tr><td>train_loss</td><td>0.60026</td></tr><tr><td>val_loss</td><td>1.61741</td></tr><tr><td>val_mean_diff</td><td>1.01077</td></tr><tr><td>val_std_diff</td><td>0.17362</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_4</strong> at: <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/akae24r0' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/akae24r0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240303_005148-akae24r0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/cs/student/projects1/2021/izaffar/FYP/FYP-MUL/model/wandb/run-20240303_005302-l7hlftyl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/l7hlftyl' target=\"_blank\">fold_5</a></strong> to <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/l7hlftyl' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/l7hlftyl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000 - Train: 443.582, Val: 347.687\n",
            "Epoch 001 - Train: 251.415, Val: 193.520\n",
            "Epoch 002 - Train: 121.103, Val: 85.995\n",
            "Epoch 003 - Train: 50.762, Val: 35.666\n",
            "Epoch 004 - Train: 22.772, Val: 16.085\n",
            "Epoch 005 - Train: 10.915, Val: 8.071\n",
            "Epoch 006 - Train: 8.101, Val: 4.929\n",
            "Epoch 007 - Train: 6.639, Val: 4.528\n",
            "Epoch 008 - Train: 3.734, Val: 2.651\n",
            "Epoch 009 - Train: 3.057, Val: 2.552\n",
            "Epoch 010 - Train: 2.392, Val: 1.804\n",
            "Epoch 011 - Train: 2.469, Val: 1.434\n",
            "Epoch 012 - Train: 2.504, Val: 2.498\n",
            "Epoch 013 - Train: 1.528, Val: 1.590\n",
            "Epoch 014 - Train: 1.955, Val: 1.160\n",
            "Epoch 015 - Train: 2.124, Val: 1.336\n",
            "Epoch 016 - Train: 1.374, Val: 1.148\n",
            "Epoch 017 - Train: 0.832, Val: 0.995\n",
            "Epoch 018 - Train: 1.656, Val: 1.351\n",
            "Epoch 019 - Train: 1.534, Val: 1.022\n",
            "Epoch 020 - Train: 1.732, Val: 0.873\n",
            "Epoch 021 - Train: 1.355, Val: 1.102\n",
            "Epoch 022 - Train: 0.956, Val: 1.064\n",
            "Epoch 023 - Train: 0.806, Val: 1.058\n",
            "Epoch 024 - Train: 1.082, Val: 1.178\n",
            "Epoch 025 - Train: 1.043, Val: 1.311\n",
            "Epoch 026 - Train: 1.536, Val: 0.867\n",
            "Epoch 027 - Train: 1.033, Val: 0.836\n",
            "Epoch 028 - Train: 1.318, Val: 1.440\n",
            "Epoch 029 - Train: 1.411, Val: 0.498\n",
            "Epoch 030 - Train: 0.654, Val: 0.602\n",
            "Epoch 031 - Train: 1.197, Val: 0.715\n",
            "Epoch 032 - Train: 0.932, Val: 1.120\n",
            "Epoch 033 - Train: 0.883, Val: 0.843\n",
            "Epoch 034 - Train: 1.194, Val: 0.956\n",
            "Epoch 035 - Train: 1.345, Val: 0.957\n",
            "Epoch 036 - Train: 1.065, Val: 1.126\n",
            "Epoch 037 - Train: 0.971, Val: 0.586\n",
            "Epoch 038 - Train: 0.604, Val: 0.492\n",
            "Epoch 039 - Train: 0.827, Val: 0.684\n",
            "Epoch 040 - Train: 0.831, Val: 0.763\n",
            "Epoch 041 - Train: 0.921, Val: 0.818\n",
            "Epoch 042 - Train: 0.714, Val: 0.938\n",
            "Epoch 043 - Train: 0.530, Val: 0.808\n",
            "Epoch 044 - Train: 0.495, Val: 0.777\n",
            "Epoch 045 - Train: 0.388, Val: 0.720\n",
            "Epoch 046 - Train: 0.383, Val: 0.739\n",
            "Epoch 047 - Train: 0.610, Val: 0.591\n",
            "Epoch 048 - Train: 0.412, Val: 0.521\n",
            "Epoch 049 - Train: 0.347, Val: 0.589\n",
            "Metrics for 21 test images - Mean Diff: 0.7342692613601685mm, Std Diff: 1.4441674947738647mm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>mean_diff</td><td>▁</td></tr><tr><td>std_diff</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_diff</td><td>█▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_std_diff</td><td>▆█▄▇▃▃▃▁▂▂▂▁▂▂▂▂▂▁▂▂▁▁▂▂▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>mean_diff</td><td>0.73427</td></tr><tr><td>std_diff</td><td>1.44417</td></tr><tr><td>train_loss</td><td>0.34742</td></tr><tr><td>val_loss</td><td>0.5894</td></tr><tr><td>val_mean_diff</td><td>0.64983</td></tr><tr><td>val_std_diff</td><td>0.21047</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_5</strong> at: <a href='https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/l7hlftyl' target=\"_blank\">https://wandb.ai/imaad-zaffar-ucl/fyp-mul/runs/l7hlftyl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240303_005302-l7hlftyl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
